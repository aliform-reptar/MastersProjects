{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641ee62a-04f5-48df-8669-9359f3a3b8cd",
   "metadata": {},
   "source": [
    "Problem Solving Task: Yeast Dataset\n",
    "- Analyse feature importance in protein presence\n",
    "- Predict for presence of protein using supervised learning models\n",
    "- Predict for presence of protein using emsemble learning models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76328ee",
   "metadata": {},
   "source": [
    "Yeast Dataset Information\n",
    "1. mcg: McGeoch's method for signal sequence recognition. \n",
    "2. gvh: von Heijne's method for signal sequence recognition. \n",
    "3. alm: Score of the ALOM membrane spanning region prediction program. \n",
    "4. mit: Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins. \n",
    "5. erl: Presence of \"HDEL\" substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute. \n",
    "6. pox: Peroxisomal targeting signal in the C-terminus. \n",
    "7. vac: Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins. \n",
    "8. nuc: Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins. \n",
    "9. class: Presence or absence of protein {positive, negative}. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6e276-6539-4f67-902f-538bf656273c",
   "metadata": {},
   "source": [
    "**ANALYSE THE IMPORTANCE OF THE FEATURES IN PREDICTING THE PRESENCE OR ABSENCE OF THE PROTEIN USING TWO DIFFERENT APPROACHES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e45c7d53-1864-4894-8469-6f4294fcff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcg</th>\n",
       "      <th>gvh</th>\n",
       "      <th>alm</th>\n",
       "      <th>mit</th>\n",
       "      <th>erl</th>\n",
       "      <th>pox</th>\n",
       "      <th>vac</th>\n",
       "      <th>nuc</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.25</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mcg   gvh   alm   mit  erl  pox   vac   nuc     class\n",
       "0  0.51  0.40  0.56  0.17  0.5  0.5  0.49  0.22  negative\n",
       "1  0.40  0.39  0.60  0.15  0.5  0.0  0.58  0.30  negative\n",
       "2  0.40  0.42  0.57  0.35  0.5  0.0  0.53  0.25  negative\n",
       "3  0.46  0.44  0.52  0.11  0.5  0.0  0.50  0.22  negative\n",
       "4  0.47  0.39  0.50  0.11  0.5  0.0  0.49  0.40  negative"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "#import data as dataframe\n",
    "yeastdata = pd.read_csv('yeast.csv') \n",
    "\n",
    "#set the names of columns\n",
    "feature_cols = ['mcg', 'gvh', 'alm','mit','erl','pox','vac','nuc','class']\n",
    "\n",
    "#print first 4 rows and head of yeast data table\n",
    "yeastdata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81292591-c1e6-4d72-906f-66df42af115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   negative  positive\n",
      "0      True     False\n",
      "1      True     False\n",
      "2      True     False\n",
      "3      True     False\n",
      "4      True     False\n",
      "   positive   mcg   gvh   alm   mit  erl  pox   vac   nuc\n",
      "0     False  0.51  0.40  0.56  0.17  0.5  0.5  0.49  0.22\n",
      "1     False  0.40  0.39  0.60  0.15  0.5  0.0  0.58  0.30\n",
      "2     False  0.40  0.42  0.57  0.35  0.5  0.0  0.53  0.25\n",
      "3     False  0.46  0.44  0.52  0.11  0.5  0.0  0.50  0.22\n",
      "4     False  0.47  0.39  0.50  0.11  0.5  0.0  0.49  0.40\n"
     ]
    }
   ],
   "source": [
    "#convert class data into binary so that it can be used for machine learning\n",
    "\n",
    "#make dummy columns positive and negativae with binary values\n",
    "posnegdummies = pd.get_dummies(yeastdata[\"class\"])\n",
    "\n",
    "#print head of binary values dummy table to check data\n",
    "print(posnegdummies.head())\n",
    "\n",
    "#drop the positive axis from the posnegdummies table\n",
    "posdummies = posnegdummies.drop([\"negative\"], axis=1)\n",
    "\n",
    "#drop the class column from the yeastdata table\n",
    "yeast = yeastdata.drop([\"class\"], axis=1)\n",
    "\n",
    "#concatenate yeast and posdummies to make a new table\n",
    "yeastML = pd.concat((posdummies, yeast), axis=1)\n",
    "\n",
    "#print head yeastML tbale to check it is ready for machine learning\n",
    "print(yeastML.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26287f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Spearman Correlation\n",
      "mcg              0.389364\n",
      "gvh              0.323893\n",
      "alm             -0.401148\n",
      "mit              0.151903\n",
      "erl              0.033410\n",
      "pox             -0.014653\n",
      "vac              0.084956\n",
      "nuc             -0.033274\n"
     ]
    }
   ],
   "source": [
    "#calculate spearman correlation between positive and each other feature, print as dataframe\n",
    "correlation_df = pd.DataFrame({\n",
    "    col: [yeastML[col].corr(yeastML['positive'], method='spearman')] \n",
    "    for col in yeastML.columns if col != 'positive'\n",
    "}).T\n",
    "correlation_df.columns = ['Spearman Correlation']\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ab3ad",
   "metadata": {},
   "source": [
    "Spearman calculated based on ranking. Spearman correlation suggests strongest relationship  to the presence of protein is features alm, mcg then gvh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cc7f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pearson Correlation\n",
      "mcg             0.535772\n",
      "gvh             0.387440\n",
      "alm            -0.480789\n",
      "mit             0.141470\n",
      "erl             0.033410\n",
      "pox            -0.014653\n",
      "vac             0.050715\n",
      "nuc            -0.038384\n"
     ]
    }
   ],
   "source": [
    "#calculate Pearson correlation between positive and each other feature, print as dataframe\n",
    "correlation_df = pd.DataFrame({\n",
    "    col: [yeastML[col].corr(yeastML['positive'], method='pearson')] \n",
    "    for col in yeastML.columns if col != 'positive'\n",
    "}).T\n",
    "correlation_df.columns = ['Pearson Correlation']\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768dabf",
   "metadata": {},
   "source": [
    "Pearson correlation is strength of linear relationship. Pearson suggests strongest relationship to the presence of protein is features mcg, alm and then gvh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e8768-7798-486b-b5bd-fd93c78c92e9",
   "metadata": {},
   "source": [
    "**CREATE THREE SUPERVISED LEARNING MODELS FOR PREDICTING PRESENCE OR ABSENCE OF PROTEIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82e3ea-562e-4437-ad22-5d3b9456bc51",
   "metadata": {},
   "source": [
    "\n",
    "PART 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb332ce2-86fa-44d2-a1ef-796760ffff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308, 8)\n",
      "(308,)\n",
      "(103, 8)\n",
      "(103,)\n",
      "(103, 8)\n",
      "(103,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data into test, train, validation sets: 70% train, 15% test, 15% validate\n",
    "\n",
    "#import test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#split existing data into X variables and y variables\n",
    "#set x variables as all columns except the positive column from the yeastML dataset\n",
    "X = yeastML.drop(columns = ['positive'])\n",
    "#set y variable as the positive column from the yeastML dataset\n",
    "y = yeastML['positive']\n",
    "\n",
    "#SPLIT 1 - split into training set (60%) and remainder of data (40%)\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.6)\n",
    "\n",
    "#SPLIT 2 - split remaining data (data not used in training set) into testing set (50% of 40%) and validation set data (50% of 40%)\n",
    "#0.5 means 50% of the remaining data in X and y will be a testing set, 20% of the original dataset\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "#this leave 50% of the remaining data as in the validation set.\n",
    "\n",
    "print(X_train.shape), print(y_train.shape)\n",
    "#print the shape of the training data\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "#print the shape of the validation data\n",
    "print(X_test.shape), print(y_test.shape)\n",
    "#print the shape of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a0437",
   "metadata": {},
   "source": [
    "\n",
    "PART 2: Set Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f42329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gridsearch from sklearn for hyperparameter tuning with validation data\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#import sklearn metrics to determinemodel performance\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db20e35a-a8ce-4230-81e0-11b196539410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.941736694677871\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Logistic Regression HYPERPARAMETERS\n",
    "\n",
    "#import logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "#initiate logistic regression model and name it log\n",
    "log = LogisticRegression() \n",
    "\n",
    "#define the parameter grid to search, best combination of C values, penalty types l1 and l2 and solver type of libinear (as supports l1 and l2 penalty types).\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'solver': ['liblinear'] \n",
    "}\n",
    "\n",
    "#initiate gridsearch with log model and the parameters set in param_grid\n",
    "grid = GridSearchCV(log, param_grid, cv=3)\n",
    "\n",
    "#fit the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best knn hyperparameters from the grid search\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of log method\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5456491a-7333-4133-bcfb-d20b404f4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.9613445378151261\n"
     ]
    }
   ],
   "source": [
    "#Model 2: K Nearest Neighbours with gridsearch to determine best hyperparameters\n",
    "\n",
    "#import nearest neighbours model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#initiate K nearest neighbours model and name it knn\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#define the parameter grid to search, best combination of 1-10 neighbours, weighting strategy and distance metrics\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 10), \n",
    "    'weights': ['uniform', 'distance'], \n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']  \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, cv=3)\n",
    "#initiate gridsearch with knn model and the parameters set in param_grid\n",
    "\n",
    "#fit the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best knn hyperparameers from the grid search\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of knn method\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c99acdf6-32e1-46e5-b27b-00f3e1858fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.8834733893557423\n"
     ]
    }
   ],
   "source": [
    "#Model 3: SVM with gridsearch to determine best hyperparameters\n",
    "\n",
    "#import svm \n",
    "from sklearn import svm\n",
    "\n",
    "#initiate svm model and name it svm\n",
    "svm = svm.SVC()\n",
    "\n",
    "#set parameters for gridsearch - search best combinations of rbf and linear kernal, as well as best gamma value between 0.0001 and 0.1\n",
    "param_grid = {\n",
    "    'kernel':('rbf', 'linear'),\n",
    "    'gamma':[0.1, 0.0001]\n",
    "    }\n",
    "\n",
    "#initiate gridsearch with svm model and the parameters set in param_grid\n",
    "grid = GridSearchCV(svm, param_grid, cv=3)\n",
    "\n",
    "# fitting the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best hyperparameers from the grid search\n",
    "print(grid_search.best_params_)\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of svm method\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3192c4a-5438-4c25-a997-624fe3b72eff",
   "metadata": {},
   "source": [
    "PART 3: Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e1c8e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print metrics\n",
    "def print_model_metrics(y_true, y_pred, model_name):\n",
    "    print(f\"\\n{model_name} Model Metrics:\")\n",
    "    print(\"Accuracy: {:.2f}\".format(skm.accuracy_score(y_true, y_pred)))\n",
    "    print(\"Precision: {:.2f}\".format(skm.precision_score(y_true, y_pred, zero_division=0)))\n",
    "    print(\"Recall: {:.2f}\".format(skm.recall_score(y_true, y_pred)))\n",
    "    print(\"F1 Score: {:.2f}\".format(skm.f1_score(y_true, y_pred)))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(skm.confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "71d719d7-0472-48a4-b3d9-3bfa59de9ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Metrics:\n",
      "Accuracy: 0.95\n",
      "Precision: 0.71\n",
      "Recall: 0.62\n",
      "F1 Score: 0.67\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  2]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Logistic Regression \n",
    "\n",
    "#initiate linear regression model with hyperparameters identified above\n",
    "logtrain = LogisticRegression(C= 100, penalty = 'l2', solver ='liblinear') \n",
    "\n",
    "#fit the model using the training data\n",
    "logtrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained logistic model on the test dataset\n",
    "logpredict = logtrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the logistic regression model\n",
    "print_model_metrics(y_test, logpredict, \"Logistic Regression\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12f3e790-f3b1-48d1-a029-cf927da70d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K nearest neighbours Model Metrics:\n",
      "Accuracy: 0.95\n",
      "Precision: 0.71\n",
      "Recall: 0.62\n",
      "F1 Score: 0.67\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  2]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "#Model 2: K Nearest Neighbours \n",
    "\n",
    "#initiate  K Nearest Neighbours model with hyperparameters identified above\n",
    "knntrain = KNeighborsClassifier(metric='euclidean', n_neighbors= 3, weights='uniform')\n",
    "\n",
    "#fit the model using the training data\n",
    "knntrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained KNN model on the test dataset\n",
    "knnypredict = knntrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the K nearest neighbours model\n",
    "print_model_metrics(y_test, knnypredict, \"K nearest neighbours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad632699-f081-427e-a0db-e0f75ce8b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Model Metrics:\n",
      "Accuracy: 0.92\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "\n",
      "Confusion Matrix:\n",
      "[[95  0]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "#Model 3: SVM \n",
    "\n",
    "#import svm \n",
    "from sklearn import svm \n",
    "\n",
    "#initiate svm model with hyperparameters identified above\n",
    "svmtrain = svm.SVC(kernel='rbf', gamma=0.1)\n",
    "\n",
    "#fit the model using the training data\n",
    "svmtrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained svm model on the test dataset\n",
    "svmypredict = svmtrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the SVM model\n",
    "print_model_metrics(y_test, svmypredict, \"SVM\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1ab9d",
   "metadata": {},
   "source": [
    "Zero values in SVM metrics are due to zero values recorded for false positives and true negatives in confusion matrix.\n",
    "SVM had highest number of false negatives and was unable to find any negative results.\n",
    "Logistic regression and K-nearest neighbours models have exactly the same results, performing better than SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad75a3-f60b-401f-8fd7-d11f8135594e",
   "metadata": {},
   "source": [
    "**BUILD THREE ENSEMBLE LEARNING MODELS FOR PREDICTING PRESENCE OR ABSENCE OF PROTEIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c259a-5285-4442-a127-fb9f25bb310a",
   "metadata": {},
   "source": [
    "\n",
    "PART 1: Determine hyperparameters of ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4f80237-2d59-495a-a371-0b36e795c984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'max_features': 'log2', 'n_estimators': 50}\n",
      "0.9417366946778712\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Random Forest\n",
    "\n",
    "#import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#initiate random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "#set parameters for gridsearch - search best combinations of max features, number of estimators and max depth\n",
    "param_grid = {\n",
    "    'max_features':('sqrt', 'log2'),\n",
    "    'n_estimators':[1, 50],\n",
    "    'max_depth':[1, 50]\n",
    "    }\n",
    "\n",
    "#Initiate gridsearch with random forest model and the parameters set in param_grid2\n",
    "grid = GridSearchCV(forest, param_grid,cv=3)\n",
    "\n",
    "# fit the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best hyperparameers from the grid search\n",
    "print(grid_search.best_params_)\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of random forest method\n",
    "print(grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff7c55a6-0155-45a2-acf0-f9f281b5ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 8, 'max_samples': 50, 'n_estimators': 50}\n",
      "0.941736694677871\n"
     ]
    }
   ],
   "source": [
    "#Model 2: Bagging Classifier with Decision Tree CLassifier as base\n",
    "\n",
    "#import bagging emsemble classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#initiate bagging classifier with decision tree\n",
    "bagging = BaggingClassifier(estimator=DecisionTreeClassifier())\n",
    "\n",
    "#set parameters for gridsearch - search best combinations of number of estimators, max-samples and max features\n",
    "param_grid = {\n",
    "    'n_estimators':[1, 50],\n",
    "    'max_samples':[1, 50],\n",
    "    'max_features':[1, 8]\n",
    "    }\n",
    "\n",
    "#Initiate gridsearch with bagging model and the parameters set in param_grid\n",
    "grid = GridSearchCV(bagging, param_grid, cv=3)\n",
    "\n",
    "# fitting the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best hyperparameers from the grid search\n",
    "print(grid_search.best_params_)\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of bagging method\n",
    "print(grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1507910e-3bc2-46f7-9048-bd2fd1560fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 1}\n",
      "0.8829131652661064\n"
     ]
    }
   ],
   "source": [
    "#Model 3: Boosting Classifier with Decision Tree CLassifier as base\n",
    "\n",
    "#import adaboost emsemble classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#initiate adaboost classifier with decision tree\n",
    "boost = AdaBoostClassifier(estimator=DecisionTreeClassifier())\n",
    "\n",
    "#set parameters for gridsearch - search best combinations of number of estimators, max-samples and max features\n",
    "param_grid = {\n",
    "    'n_estimators':[1, 50],\n",
    "    'learning_rate':[0.1, 100000]}\n",
    "\n",
    "#Initiate gridsearch with adaboost model and the parameters set in param_grid\n",
    "grid = GridSearchCV(boost, param_grid, cv=3)\n",
    "\n",
    "# fitting the model for grid search to the Validation data set\n",
    "grid_search = grid.fit(X_valid, y_valid)\n",
    "\n",
    "#print best hyperparameers from the grid search\n",
    "print(grid_search.best_params_)\n",
    "#print the best accuracy score from sklearn metrics with values from validation set as result of adaboost method\n",
    "print(grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d6c5c-42cc-4926-90f6-4e894d36f766",
   "metadata": {},
   "source": [
    "PART 2: Testing Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e1332563-5eb3-470f-802c-07d104fd5b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Model Metrics:\n",
      "Accuracy: 0.95\n",
      "Precision: 1.00\n",
      "Recall: 0.38\n",
      "F1 Score: 0.55\n",
      "\n",
      "Confusion Matrix:\n",
      "[[95  0]\n",
      " [ 5  3]]\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Random Forest\n",
    "\n",
    "#initiate random forest classifier with hyperparameters determined from the validation set\n",
    "foresttrain = RandomForestClassifier(max_depth=1, max_features='log2', n_estimators=50)\n",
    "\n",
    "#fit the model using the training data\n",
    "foresttrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained linear model on the test dataset\n",
    "forestypredict = foresttrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the Random Forest model\n",
    "print_model_metrics(y_test, forestypredict, \"Random Forest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64c1dda8-b1d5-4662-89b0-da6e28ff8940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging Model Metrics:\n",
      "Accuracy: 0.95\n",
      "Precision: 0.71\n",
      "Recall: 0.62\n",
      "F1 Score: 0.67\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  2]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "#Model 2 Bagging Classifier with Decision Tree Classifier as base\n",
    "\n",
    "#initiate bagging classifier with decision tree base and other hyperparameters identified from grid search\n",
    "baggingtrain = BaggingClassifier(estimator=DecisionTreeClassifier(),max_features=8,max_samples=50,n_estimators=50)\n",
    "\n",
    "#fit the model using the training data\n",
    "baggingtrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained bagging model on the test dataset\n",
    "baggingypredict = baggingtrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the Bagging model\n",
    "print_model_metrics(y_test, baggingypredict, \"Bagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f8799a4-4c6d-4dd5-8aec-6913e384021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boosting Model Metrics:\n",
      "Accuracy: 0.95\n",
      "Precision: 0.71\n",
      "Recall: 0.62\n",
      "F1 Score: 0.67\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  2]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "#Model 3: Boosting Classifier with Decision Tree Classifier as base\n",
    "\n",
    "#initiate adaboost classifier with decision tree base and other hyperparameters identified from grid search\n",
    "boosttrain = AdaBoostClassifier(estimator=DecisionTreeClassifier(),learning_rate=0.1,n_estimators=1)\n",
    "\n",
    "#fit the model using the training data\n",
    "boosttrain.fit(X_train, y_train)\n",
    "\n",
    "#run the trained adaboost model on the test dataset\n",
    "boostypredict = baggingtrain.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the Boosting model\n",
    "print_model_metrics(y_test, boostypredict, \"Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc0af2-ea4b-47db-ad58-8cd8c1284e3f",
   "metadata": {},
   "source": [
    "PART 3: Is it possible to build ensemble models with classifiers other than decision tree? Explain with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2948e16b-d4fd-461a-872c-697e737366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging with KNN estimator Model Metrics:\n",
      "Accuracy: 0.97\n",
      "Precision: 0.86\n",
      "Recall: 0.75\n",
      "F1 Score: 0.80\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94  1]\n",
      " [ 2  6]]\n"
     ]
    }
   ],
   "source": [
    "#Ensemble bagging model with KNN classifier\n",
    "\n",
    "#initiate bagging classifier with KNN base\n",
    "baggingtrain2 = BaggingClassifier(estimator=KNeighborsClassifier(n_neighbors=1))\n",
    "#initiate bagging classifier with KNN base\n",
    "\n",
    "#fit the model using the training data\n",
    "baggingtrain2.fit(X_train, y_train)\n",
    "\n",
    "#run the trained bagging model on the test dataset\n",
    "baggingypredict2 = baggingtrain2.predict(X_test)\n",
    "\n",
    "#calculate the metrics fot the Boosting model\n",
    "print_model_metrics(y_test, baggingypredict2, \"Bagging with KNN estimator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b5671",
   "metadata": {},
   "source": [
    "Random Forest Classifier had the highest precision rating (1), as no false positive results were made by the model. However, many false negatives made the recall value and thus also the F1 score low for this model.\n",
    "Using a Bagging and Boosting emsemble method with Decision Tree Classifier gave exactly the same results metrics and confusion matrix results as with a Logistic Regression and  and K-nearest Neighbours methods.\n",
    "The method that performed best (as a whole as well as the best ensemble method) was using a Bagging ensemble with a K-nearest Neighbours base. This method had the highest number of true results, with only 3 total false results from the  testing dataset (103 in size). These results led to highest accuracy, precision, recall and f1 scores for this method.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
